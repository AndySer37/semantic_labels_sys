{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "if \"/opt/ros/kinetic/lib/python2.7/dist-packages\" in sys.path:\n",
    "    sys.path.remove(\"/opt/ros/kinetic/lib/python2.7/dist-packages\")\n",
    "import cv2\n",
    "import struct\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import os \n",
    "\n",
    "from network.textnet import TextNet\n",
    "from util.detection import TextDetector\n",
    "from util.augmentation import BaseTransform\n",
    "from util.config import config as cfg, update_config, print_config\n",
    "from util.option import BaseOptions\n",
    "from util.visualize import visualize_detection\n",
    "from util.misc import to_device, mkdirs, rescale_result\n",
    "from rotate_input import rotate_cv, rotate_back, rotate_back_change_h_w\n",
    "from PIL import Image as Im\n",
    "import tools.utils as utils\n",
    "import tools.dataset as dataset\n",
    "from models.moran import MORAN\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Oct  8 2019, 13:06:37) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_commodity(path):\n",
    "    _list = []\n",
    "    for line in open(path, \"r\"):\n",
    "        line = line.rstrip('\\n')\n",
    "        _list.append(line)\n",
    "    print (\"Finish reading list\")\n",
    "    return _list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading list\n"
     ]
    }
   ],
   "source": [
    "alphabet = '0:1:2:3:4:5:6:7:8:9:a:b:c:d:e:f:g:h:i:j:k:l:m:n:o:p:q:r:s:t:u:v:w:x:y:z:$' \n",
    "cuda_use = torch.cuda.is_available()\n",
    "means = (0.485, 0.456, 0.406)\n",
    "stds = (0.229, 0.224, 0.225)\n",
    "bbox_thres = 1500\n",
    "color_map = [(255,0,0),(0,255,0),(0,0,255),(255,255,0),(255,255,255)] # 0 90 180 270 noise\n",
    "commodity_list = read_commodity(\"./config/commodity_list.txt\")\n",
    "\n",
    "csv_file = \"./val.csv\"\n",
    "data_list = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andyser/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/andyser/code/text_detect_recognize/semantic_lables_sys/thesis/benchmark/models/asrn_res.py:234: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight, mode='fan_out', a=0)\n",
      "/home/andyser/code/text_detect_recognize/semantic_lables_sys/thesis/benchmark/models/asrn_res.py:236: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.weight, 1)\n",
      "/home/andyser/code/text_detect_recognize/semantic_lables_sys/thesis/benchmark/models/asrn_res.py:237: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.bias, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ./model/textsnake_vgg_0.pth\n"
     ]
    }
   ],
   "source": [
    "snake_network = TextNet(is_training=False, backbone='vgg')\n",
    "if cuda_use:\n",
    "    snake_network = snake_network.cuda()\n",
    "    cuda_flag = True\n",
    "    MORAN_network = MORAN(1, len(alphabet.split(':')), 256, 32, 100, BidirDecoder=True, CUDA=cuda_flag)\n",
    "    MORAN_network = MORAN_network.cuda()\n",
    "    MORAN_state_dict = torch.load(os.path.join(\"./model/\", \"demo.pth\"))\n",
    "else:\n",
    "    MORAN_network = MORAN(1, len(alphabet.split(':')), 256, 32, 100, BidirDecoder=True, inputDataType='torch.FloatTensor', CUDA=cuda_flag)\n",
    "    MORAN_state_dict = torch.load(os.path.join(\"./model/\", \"demo.pth\"), map_location='cpu')\n",
    "MORAN_state_dict_rename = OrderedDict()\n",
    "for k, v in MORAN_state_dict.items():\n",
    "    name = k.replace(\"module.\", \"\") # remove `module.`\n",
    "    MORAN_state_dict_rename[name] = v\n",
    "    \n",
    "MORAN_network.load_state_dict(MORAN_state_dict_rename)\n",
    "converter = utils.strLabelConverterForAttention(alphabet, ':')\n",
    "transformer = dataset.resizeNormalize((100, 32))\n",
    "\n",
    "for p in MORAN_network.parameters():\n",
    "    p.requires_grad = False\n",
    "snake_network.load_model(os.path.join(\"./model/\", \"textsnake_vgg_0.pth\"))   \n",
    "MORAN_network.eval()\n",
    "snake_network.eval()\n",
    "detector = TextDetector(snake_network, tr_thresh=0.6, tcl_thresh=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_predict(img):\n",
    "    # # Preprocessing\n",
    "    (rows, cols, channels) = img.shape\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    x = image.astype(np.float32)\n",
    "    x = (x / 255 - means) / stds\n",
    "    x = x.astype(np.float32)\n",
    "    x = x[:, :, ::-1].copy()\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1)\n",
    "    x = Variable(x.unsqueeze(0)) \n",
    "    if cuda_use:\n",
    "        x = x.cuda()\n",
    "    contours, output = detector.detect(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    print (\"Text Detection Time : {}\".format(end - start))\n",
    "\n",
    "    image, contours = rescale_result(image, contours, rows, cols)\n",
    "    img_viz = visualize_detection(image, contours)\n",
    "\n",
    "    return img_viz, contours\n",
    "\n",
    "## bbox = [xmin, xmax, ymin, ymax]\n",
    "def recog_predict(bbox, contour, img, img_vis, mask, rot=0):\n",
    "    # # Preprocessing\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    (rows, cols, channels) = img.shape\n",
    "    \n",
    "    if (bbox[3] - bbox[2]) * (bbox[1] - bbox[0]) < bbox_thres:\n",
    "        return img, mask\n",
    "    start = time.time()\n",
    "    image = gray[bbox[2]:bbox[3], bbox[0]:bbox[1]]\n",
    "\n",
    "    image = Im.fromarray(image) \n",
    "    image = transformer(image)\n",
    "\n",
    "    if cuda_use:\n",
    "        image = image.cuda()\n",
    "    image = image.view(1, *image.size())\n",
    "    image = Variable(image)\n",
    "    text = torch.LongTensor(1 * 5)\n",
    "    length = torch.IntTensor(1)\n",
    "    text = Variable(text)\n",
    "    length = Variable(length)\n",
    "\n",
    "    max_iter = 20\n",
    "    t, l = converter.encode('0'*max_iter)\n",
    "    utils.loadData(text, t)\n",
    "    utils.loadData(length, l)\n",
    "    output = MORAN_network(image, length, text, text, test=True, debug=True)\n",
    "\n",
    "    preds, preds_reverse = output[0]\n",
    "    demo = output[1]\n",
    "\n",
    "    _, preds = preds.max(1)\n",
    "    _, preds_reverse = preds_reverse.max(1)\n",
    "\n",
    "    sim_preds = converter.decode(preds.data, length.data)\n",
    "    sim_preds = sim_preds.strip().split('$')[0]\n",
    "    sim_preds_reverse = converter.decode(preds_reverse.data, length.data)\n",
    "    sim_preds_reverse = sim_preds_reverse.strip().split('$')[0]\n",
    "\n",
    "    # print('\\nResult:\\n' + 'Left to Right: ' + sim_preds + '\\nRight to Left: ' + sim_preds_reverse + '\\n\\n')\n",
    "    print (\"Text Recognize Time : {}\".format(time.time() - start))\n",
    "\n",
    "    if sim_preds in commodity_list:\n",
    "        cv2.rectangle(img, (bbox[0], bbox[2]),(bbox[1], bbox[3]), color_map[rot], 3)\n",
    "        cv2.putText(img, sim_preds, (bbox[0], bbox[2]), 0, 1, (0, 255, 255),3)\n",
    "        cv2.fillConvexPoly(mask, contour, commodity_list.index(sim_preds) + rot*len(commodity_list) + 1)\n",
    "    else:\n",
    "        correct, conf, _bool = conf_of_word(sim_preds)\n",
    "\n",
    "        if _bool:\n",
    "            cv2.putText(img, correct + \"{:.2f}\".format(conf), (bbox[0], bbox[2]), 0, 1, (0, 255, 255),3)\n",
    "            cv2.rectangle(img, (bbox[0], bbox[2]),(bbox[1], bbox[3]), (255, 255, 255), 2)\n",
    "            cv2.fillConvexPoly(mask, _cont, commodity_list.index(correct) + rot*len(commodity_list) + 1)\n",
    "        else:\n",
    "            cv2.putText(img, sim_preds, (bbox[0], bbox[2]), 0, 1, (0, 0, 0),3)\n",
    "            cv2.rectangle(img, (bbox[0], bbox[2]),(bbox[1], bbox[3]), (0, 0, 0), 2)\n",
    "        print (sim_preds, conf, correct, rot*90)\n",
    "\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_of_word(target):\n",
    "    total = np.zeros(len(commodity_list))\n",
    "\n",
    "    for i in range(1, len(commodity_list)):\n",
    "        \n",
    "        err = 0  ## error \n",
    "        _len = len(commodity_list[i])\n",
    "        arr = -10 * np.ones(_len)\n",
    "        for j in range(len(target)):\n",
    "            index = commodity_list[i].find(target[j])\n",
    "            if index == -1:\n",
    "                err += 1\n",
    "            else:\n",
    "                upper = arr[index+1] if index != _len - 1 else -10\n",
    "                if arr[index] == -10 and upper == -10:\n",
    "                    arr[index] = j\n",
    "                else:\n",
    "                    index = commodity_list[i].find(target[j], index + 1)\n",
    "                    while index != -1:\n",
    "                        lower = arr[index-1] if index != 0 else -10\n",
    "                        upper = arr[index+1] if index != _len - 1 else -10\n",
    "                        if (arr[index] - lower) == 1 or (upper - arr[index]) == 1:\n",
    "                            index = commodity_list[i].find(target[j], index + 1)\n",
    "                        else:\n",
    "                            arr[index] = j\n",
    "                            break\n",
    "\n",
    "        score = 0   # score for word \n",
    "        for j in range(_len - 1):\n",
    "            if arr[j+1] - arr[j] == 1:\n",
    "                score += 1\n",
    "        total[i] = float(score) / (_len + err - 1)\n",
    "\n",
    "    return commodity_list[np.argmax(total)], np.max(total), np.max(total) > 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj(_img, _level = 8):\n",
    "    (colomn, row) = _img.shape\n",
    "    _count = 0\n",
    "    _pixel_pair = []\n",
    "    label = np.zeros((colomn,row),dtype = np.uint8)\n",
    "    for i in range(colomn):\n",
    "        for j in range(row):\n",
    "            if (_img[i,j] == 1 and label[i,j] == 0):\n",
    "                _pixel_pair.append([i,j])\n",
    "                _count += 1\n",
    "            while len(_pixel_pair) != 0:\n",
    "                pair = _pixel_pair.pop()\n",
    "                a = pair[1] + 1\n",
    "                b = pair[1] - 1\n",
    "                c = pair[0] + 1\n",
    "                d = pair[0] - 1\n",
    "                if a == 640 : a -= 1\n",
    "                if b == -1  : b += 1\n",
    "                if c == 480 : c -= 1\n",
    "                if d == -1  : d += 1\n",
    "\n",
    "                if _img[pair[0],a] == 1 and label[pair[0],a] == 0:\n",
    "                    _pixel_pair.append([pair[0],a])\n",
    "                if _img[pair[0],b] == 1 and label[pair[0],b] == 0:\n",
    "                    _pixel_pair.append([pair[0],b])\n",
    "                if _img[c,pair[1]] == 1 and label[c,pair[1]] == 0:\n",
    "                    _pixel_pair.append([c,pair[1]])\n",
    "                if _img[d,pair[1]] == 1 and label[d,pair[1]] == 0:\n",
    "                    _pixel_pair.append([d,pair[1]])\n",
    "                if _level == 8:\n",
    "                    if _img[c,a] == 1 and label[c,a] == 0:\n",
    "                        _pixel_pair.append([c,a])\n",
    "                    if _img[d,a] == 1 and label[d,a] == 0:\n",
    "                        _pixel_pair.append([d,a])\n",
    "                    if _img[d,b] == 1 and label[d,b] == 0:\n",
    "                        _pixel_pair.append([d,b])\n",
    "                    if _img[c,b] == 1 and label[c,b] == 0:\n",
    "                        _pixel_pair.append([c,b])\n",
    "                label[pair[0],pair[1]] = _count\n",
    "\n",
    "    print(\"Num of classes for connected components : \", _count)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Detection Time : 0.0762178897857666\n",
      "Text Detection Time : 0.07665896415710449\n",
      "Text Recognize Time : 0.035015106201171875\n",
      "Text Detection Time : 0.07288599014282227\n",
      "Text Detection Time : 0.0761101245880127\n",
      "Text Recognize Time : 0.03964996337890625\n",
      "oiin 0.14285714285714285 raisins 270\n",
      "Num of classes for connected components :  1\n",
      "Num of classes for connected components :  1\n",
      "0.75973669775096 1385.0 1823.0\n",
      "[0 4]\n",
      "[0. 4.]\n",
      "109\n",
      "Text Detection Time : 0.0713496208190918\n",
      "Text Recognize Time : 0.03558349609375\n",
      "Text Detection Time : 0.07024979591369629\n",
      "Text Recognize Time : 0.037142038345336914\n",
      "Text Detection Time : 0.07074928283691406\n",
      "Text Detection Time : 0.07012820243835449\n",
      "Text Recognize Time : 0.03444314002990723\n",
      "olin 0.0 crayons 270\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-75df236b38b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mmask_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mlabel_sor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mlabel_tar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mpix_sor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_sor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d2b9da71e396>\u001b[0m in \u001b[0;36madj\u001b[0;34m(_img, _level)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolomn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolomn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0m_pixel_pair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TP = np.zeros((41), dtype = np.float128)\n",
    "FP = np.zeros((41), dtype = np.float128)\n",
    "FN = np.zeros((41), dtype = np.float128)\n",
    "_sum_origin = 0\n",
    "_sum_detection = 0\n",
    "\n",
    "theta_sum = 0.\n",
    "theta_count = 0\n",
    "iou_sum = 0.\n",
    "_total_bn_up = 0\n",
    "for idx in range(len(data_list)):\n",
    "    img_name   = data_list.iloc[idx, 0]\n",
    "    img        = cv2.imread(img_name,cv2.IMREAD_UNCHANGED)\n",
    "    label_name = data_list.iloc[idx, 1]\n",
    "    label      = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n",
    "    if \"single\" not in img_name:\n",
    "        continue\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    ### Detection & Recognition\n",
    "    label[label == 246] = 0\n",
    "    \n",
    "    mask = np.zeros([img.shape[0], img.shape[1]], dtype = np.uint8)\n",
    "    img_list_0_90_180_270 = rotate_cv(img)\n",
    "    \n",
    "    for i in range(4):\n",
    "\n",
    "        predict_img, contours = region_predict(img_list_0_90_180_270[i])\n",
    "        img_bbox = img_list_0_90_180_270[i].copy()\n",
    "        img_input_vis = img_bbox.copy()\n",
    "        img_input_rec = img_bbox.copy()\n",
    "        (rows, cols, channels) = img_bbox.shape\n",
    "        mask_vis = np.zeros([rows, cols], dtype = np.uint8)\n",
    "        for _cont in contours:\n",
    "            \n",
    "#             cv2.fillConvexPoly(mask_vis, _cont, 1)\n",
    "            \n",
    "            cv2.drawContours(predict_img, [_cont], -1, color_map[i], 3)\n",
    "            cv2.rectangle(img_bbox, (min(_cont[:,0]), min(_cont[:,1])),(max(_cont[:,0]), max(_cont[:,1])), color_map[i], 3)\n",
    "            bbox = [min(_cont[:,0]),max(_cont[:,0]),min(_cont[:,1]),max(_cont[:,1])]\n",
    "            img_input_vis, mask_vis = recog_predict(bbox, _cont, img_input_rec,img_input_vis, mask_vis, rot = i)\n",
    "            \n",
    "\n",
    "        if i == 0:\n",
    "            pass\n",
    "        elif i == 1:\n",
    "            mask_vis = rotate_back_change_h_w(mask_vis, angle = -90)\n",
    "        elif i == 2:\n",
    "            mask_vis = rotate_back(mask_vis, angle = -180)\n",
    "        else:\n",
    "            mask_vis = rotate_back_change_h_w(mask_vis, angle = -270)\n",
    "        mask[mask_vis != 0] = mask_vis[mask_vis != 0]\n",
    "    \n",
    "    \n",
    "    ###########\n",
    "    #### For detection evaluation\n",
    "    \n",
    "#     mask_vis = label*mask_vis\n",
    "#     mask_vis[mask_vis != 0] = 1\n",
    "#     label[label != 0] = 1\n",
    "#     _label_1 = adj(label)\n",
    "#     _label_2 = adj(mask_vis)\n",
    "#     _sum_origin += len(np.unique(_label_1)) - 1\n",
    "#     _sum_detection += len(np.unique(_label_2)) - 1 \n",
    "    \n",
    "#     print (len(np.unique(_label_1)))\n",
    "#     print (len(np.unique(_label_2)))\n",
    "    \n",
    "    #### Confusion Matrix\n",
    "    \n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.imshow(_label_1,  cmap = \"gray\", vmin = 0, vmax = 10)\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(_label_2, cmap = \"gray\", vmin = 0, vmax = 1)\n",
    "#     plt.show()\n",
    "    \n",
    "    label = label/6\n",
    "    for j in range(11,41):\n",
    "        c = j\n",
    "        while(c>10):\n",
    "            c-=10\n",
    "        label[label == j] = c\n",
    "        mask[mask == j] = c\n",
    "        \n",
    "    #### Brandbase evaluation\n",
    "    obj_list = np.unique(label)\n",
    "    for pix in obj_list:\n",
    "        if pix != 0:\n",
    "            mask_source = np.zeros([img.shape[0], img.shape[1]], dtype = np.uint8)\n",
    "            mask_target = np.zeros([img.shape[0], img.shape[1]], dtype = np.uint8)\n",
    "            \n",
    "            mask_source[label == pix] = 1\n",
    "            mask_target[mask == pix] = 1\n",
    "            \n",
    "            label_sor = adj(mask_source)\n",
    "            label_tar = adj(mask_target)\n",
    "            pix_sor = np.unique(label_sor)\n",
    "            pix_tar = np.unique(label_tar)\n",
    "            for p1 in pix_sor:\n",
    "                if p1 != 0:\n",
    "                    _total_bn_up += 1\n",
    "                    temp_sor = np.zeros([img.shape[0], img.shape[1]], dtype = np.uint8)\n",
    "                    temp_sor[label_sor == p1] = 1\n",
    "                    for p2 in pix_tar: \n",
    "                        if p2 != 0:\n",
    "                            temp_tar = np.zeros([img.shape[0], img.shape[1]], dtype = np.uint8)\n",
    "                            temp_tar[label_tar == p2] = 1\n",
    "                            \n",
    "                            kernel = np.ones((3,3),np.uint8)  \n",
    "                            temp_tar = cv2.erode(temp_tar,kernel,iterations = 1)\n",
    "                            \n",
    "                            _and_pix = float(sum(sum(np.logical_and(temp_sor, temp_tar))))\n",
    "                            _or_pix = float(sum(sum(np.logical_or(temp_sor, temp_tar))))\n",
    "                            iou_rate = _and_pix/_or_pix\n",
    "                            if iou_rate >= 0.5:\n",
    "                                contours1, _ = cv2.findContours(temp_sor, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                                contours2, _ = cv2.findContours(temp_tar, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                                \n",
    "                                rect1 = cv2.minAreaRect(contours1[0])\n",
    "                                box1 = cv2.boxPoints(rect1)\n",
    "                                box1 = np.int0(box1)\n",
    "                                \n",
    "                                rect2 = cv2.minAreaRect(contours2[0])\n",
    "                                box2 = cv2.boxPoints(rect2)\n",
    "                                box2 = np.int0(box2)  \n",
    "                                \n",
    "                                theta1 = np.arctan2(box1[0][1]-box1[1][1], box1[0][0]-box1[1][0])/3.1415926*180\n",
    "                                theta2 = np.arctan2(box2[0][1]-box2[1][1], box2[0][0]-box2[1][0])/3.1415926*180\n",
    "                                \n",
    "                                dis = np.absolute(theta2-theta1)\n",
    "                                if dis > 45:\n",
    "                                    dis = 90 - dis \n",
    "                                \n",
    "                                theta_count += 1\n",
    "                                theta_sum += dis\n",
    "                                iou_sum += iou_rate\n",
    "#                                 print (dis)\n",
    "                                \n",
    "#                                 plt.figure()\n",
    "#                                 plt.imshow(img)\n",
    "#                                 plt.show()\n",
    "\n",
    "#                                 plt.figure()\n",
    "#                                 plt.imshow(temp_sor,  cmap = \"brg\", vmin = 0, vmax = 1)\n",
    "#                                 plt.show()\n",
    "\n",
    "#                                 plt.figure()\n",
    "#                                 plt.imshow(temp_tar, cmap = \"Reds\", vmin = 0, vmax = 1)\n",
    "#                                 plt.show()                                \n",
    "                                print (iou_rate,_and_pix,_or_pix)\n",
    "                            \n",
    "                    \n",
    "        \n",
    "    #####\n",
    "    \n",
    "    print (np.unique(mask))\n",
    "    print (np.unique(label))\n",
    "    target = mask.reshape(480 * 640)\n",
    "    pred = label.reshape(480 * 640)\n",
    "\n",
    "    con_matrix = confusion_matrix(target, pred,labels = np.arange(0,41,1))\n",
    "    con_matrix[0][0] = 0\n",
    "    for i in range(0, 41):\n",
    "        for j in range(0, 41):\n",
    "            if i == j:\n",
    "                TP[i] += con_matrix[i][j]\n",
    "            if i != j:\n",
    "                FP[j] += con_matrix[i][j]\n",
    "                FN[i] += con_matrix[i][j]\n",
    "    print(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (_total_bn_up)\n",
    "print (theta_count)\n",
    "print (theta_sum / theta_count)\n",
    "print (iou_sum / theta_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "[    0.  3220.  2527. 23065. 13031. 10852.  1988.  7560. 14884. 11347.\n",
      " 16046.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.]\n",
      "[    0.     0.  2470. 12148.  5522.  4875.  4131.  6636. 16505.  4966.\n",
      "  7403.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.]\n",
      "[    0.  1374. 10070. 22898.  2109.   373.    65.  2903.  3640.  9998.\n",
      "   853.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.]\n"
     ]
    }
   ],
   "source": [
    "print (_sum_origin, _sum_detection)\n",
    "FN[0] = 0\n",
    "FP[0] = 0\n",
    "print (TP)\n",
    "print (FN)\n",
    "print (FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andyser/.local/lib/python3.5/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/andyser/.local/lib/python3.5/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/home/andyser/.local/lib/python3.5/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/andyser/.local/lib/python3.5/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "ious = TP / (TP + FN + FP)\n",
    "fscore = 2*TP / (2*TP + FN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       nan 0.70091424 0.16771753 0.3969128  0.63067467 0.67403727\n",
      " 0.32147477 0.44213112 0.42490508 0.43126449 0.66027487        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan]\n",
      "[       nan 0.82416176 0.28725702 0.56827141 0.77351379 0.80528347\n",
      " 0.4865394  0.61316355 0.59639773 0.60263423 0.79538019        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan]\n",
      "0.48503068403625244052\n",
      "0.63526025562783965713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.63735787962034154625"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (ious)\n",
    "print (fscore)\n",
    "print (sum(ious[1:11]/10))\n",
    "print (sum(fscore[1:11]/10))\n",
    "2*sum(TP) / (2*sum(TP) + sum(FN) + sum(FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4971831871217473"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IOU\n",
    "0.42548957096516391334\n",
    "0.5284407007142797736          0.5421415715311996553\n",
    "0.5044586460943159716          0.5388207442850785732\n",
    "0.38584792222682856477         0.43220951477705407327\n",
    "0.46073640775482894497         0.42075081174636323325\n",
    "0.48503068403625244052          0.49718318712174729647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-53-3f96fb624380>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-53-3f96fb624380>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    0.59014847078778318024        0.6139501681337725087\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### F-score\n",
    "0.59014847078778318024        0.6139501681337725087\n",
    "0.6852427911485629876\n",
    "0.66380536213827919965         0.66187100446803369657\n",
    "0.53617502895716315014        0.5598313305769117412\n",
    "0.6238636460835660344        0.53806262726474883533\n",
    "0.63526025562783965713         0.60057070070623645773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-6885a956a83b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-6885a956a83b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    GPU Comparison\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "GPU Comparison\n",
    "Inference Time\n",
    "Text Avg Detection Time : 0.075\n",
    "Text Avg Recognize Time : 0.034\n",
    "FCN: 0.318\n",
    "\n",
    "RAM Needed\n",
    "SnakeText : 613 Mb\n",
    "MORAN : 723 Mb\n",
    "    \n",
    "FCN : 1806 Mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text detection result\n",
    "37/50\n",
    "108/145\n",
    "115/159\n",
    "\n",
    "\n",
    "26 / 50\n",
    "3.1094410898469844\n",
    "0.6620932068194665\n",
    "\n",
    "114 / 145\n",
    "2.7959681340597156\n",
    "0.6834069119013017\n",
    "\n",
    "116 / 159\n",
    "3.33064518315082\n",
    "0.6862443603729517\n",
    "\n",
    "16 / 31\n",
    "4.157111493864659\n",
    "0.6179058317411024\n",
    "\n",
    "20 / 32\n",
    "4.653125144053842\n",
    "0.630576208541251\n",
    "\n",
    "42 / 59\n",
    "3.6050439487907227\n",
    "0.6773317515173449"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
